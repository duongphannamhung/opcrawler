Notice: This doc generated by AI

# CDC & AWS DMS: PostgreSQL ↔ S3 Replication

## Overview

This document explains how to implement bidirectional data replication between PostgreSQL and S3 using AWS Database Migration Service (DMS) with Change Data Capture (CDC) for real-time streaming and batch processing for hourly synchronization.

## Architecture

```
PostgreSQL (Source) → AWS DMS → S3 (Target) → AWS Glue/Lambda → PostgreSQL (Destination)
     |                   ↑              |                            ↑
     |                   |              |                            |
     └─── CDC Stream ────┘              └──── Batch Process ────────┘
```

## Part 1: PostgreSQL → S3 CDC Streaming

### 1.1 Prerequisites

#### PostgreSQL Configuration
```sql
-- Enable logical replication
ALTER SYSTEM SET wal_level = logical;
ALTER SYSTEM SET max_wal_senders = 10;
ALTER SYSTEM SET max_replication_slots = 10;
ALTER SYSTEM SET max_wal_size = '1GB';

-- Restart PostgreSQL and create replication slot
SELECT pg_create_logical_replication_slot('dms_slot', 'pgoutput');

-- Grant necessary permissions
CREATE USER dms_user WITH REPLICATION LOGIN PASSWORD 'secure_password';
GRANT rds_replication TO dms_user;
GRANT SELECT ON ALL TABLES IN SCHEMA public TO dms_user;
GRANT USAGE ON SCHEMA public TO dms_user;
```

#### Orders Table Setup
```sql
-- Create orders table with primary key (required for CDC)
CREATE TABLE orders (
    order_id SERIAL PRIMARY KEY,
    customer_id INTEGER NOT NULL,
    order_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    total_amount DECIMAL(10,2),
    status VARCHAR(50) DEFAULT 'pending',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Enable replica identity for CDC
ALTER TABLE orders REPLICA IDENTITY FULL;

-- Create trigger for updated_at
CREATE OR REPLACE FUNCTION update_timestamp()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = CURRENT_TIMESTAMP;
    RETURN NEW;
END;
$$ language 'plpgsql';

CREATE TRIGGER update_orders_timestamp 
    BEFORE UPDATE ON orders 
    FOR EACH ROW EXECUTE FUNCTION update_timestamp();
```

### 1.2 AWS DMS Setup for CDC Streaming

#### Create Source Endpoint (PostgreSQL)
```json
{
  "EndpointIdentifier": "postgresql-source-orders",
  "EndpointType": "source",
  "EngineName": "postgres",
  "Username": "dms_user",
  "Password": "secure_password",
  "ServerName": "your-postgres-host.amazonaws.com",
  "Port": 5432,
  "DatabaseName": "your_database",
  "PostgreSQLSettings": {
    "CaptureDdls": false,
    "MaxFileSize": 32768,
    "DatabaseName": "your_database",
    "DdlArtifactsSchema": "public",
    "ExecuteTimeout": 60,
    "FailTasksOnLobTruncation": true,
    "HeartbeatEnable": true,
    "HeartbeatSchema": "public",
    "HeartbeatFrequency": 5000,
    "SlotName": "dms_slot"
  }
}
```

#### Create Target Endpoint (S3)
```json
{
  "EndpointIdentifier": "s3-target-orders",
  "EndpointType": "target",
  "EngineName": "s3",
  "S3Settings": {
    "ServiceAccessRoleArn": "arn:aws:iam::123456789012:role/dms-s3-role",
    "BucketName": "your-orders-bucket",
    "BucketFolder": "orders-cdc/",
    "CdcPath": "cdc/",
    "CompressionType": "gzip",
    "CsvDelimiter": ",",
    "CsvRowDelimiter": "\\n",
    "DataFormat": "parquet",
    "DatePartitionEnabled": true,
    "DatePartitionSequence": "yyyymmddhh",
    "IncludeOpForFullLoad": true,
    "ParquetTimestampInMillisecond": true,
    "TimestampColumnName": "dms_timestamp"
  }
}
```

### 1.3 Replication Task Configuration

```json
{
  "ReplicationTaskIdentifier": "orders-cdc-stream",
  "SourceEndpointArn": "arn:aws:dms:region:account:endpoint:postgresql-source-orders",
  "TargetEndpointArn": "arn:aws:dms:region:account:endpoint:s3-target-orders",
  "ReplicationInstanceArn": "arn:aws:dms:region:account:rep:dms-instance",
  "MigrationType": "cdc",
  "TableMappings": {
    "rules": [
      {
        "rule-type": "selection",
        "rule-id": "1",
        "rule-name": "orders-table",
        "object-locator": {
          "schema-name": "public",
          "table-name": "orders"
        },
        "rule-action": "include"
      },
      {
        "rule-type": "transformation",
        "rule-id": "2",
        "rule-name": "add-metadata",
        "rule-target": "table",
        "object-locator": {
          "schema-name": "public",
          "table-name": "orders"
        },
        "rule-action": "add-column",
        "value": "dms_operation",
        "expression": "operation"
      }
    ]
  },
  "ReplicationTaskSettings": {
    "TargetMetadata": {
      "TargetSchema": "",
      "SupportLobs": true,
      "FullLobMode": false,
      "LobChunkSize": 0,
      "LimitedSizeLobMode": true,
      "LobMaxSize": 32,
      "InlineLobMaxSize": 0,
      "LoadMaxFileSize": 0,
      "ParallelLoadThreads": 0,
      "ParallelLoadBufferSize": 0,
      "BatchApplyEnabled": false,
      "TaskRecoveryTableEnabled": false
    },
    "FullLoadSettings": {
      "TargetTablePrepMode": "DO_NOTHING",
      "CreatePkAfterFullLoad": false,
      "StopTaskCachedChangesApplied": false,
      "StopTaskCachedChangesNotApplied": false,
      "MaxFullLoadSubTasks": 8,
      "TransactionConsistencyTimeout": 600,
      "CommitRate": 10000
    },
    "Logging": {
      "EnableLogging": true,
      "LogComponents": [
        {
          "Id": "SOURCE_UNLOAD",
          "Severity": "LOGGER_SEVERITY_DEFAULT"
        },
        {
          "Id": "TARGET_LOAD",
          "Severity": "LOGGER_SEVERITY_DEFAULT"
        }
      ]
    },
    "ControlTablesSettings": {
      "ControlSchema": "",
      "HistoryTimeslotInMinutes": 5,
      "HistoryTableEnabled": true,
      "SuspendedTablesTableEnabled": true,
      "StatusTableEnabled": true
    },
    "StreamBufferSettings": {
      "StreamBufferCount": 3,
      "StreamBufferSizeInMB": 8,
      "CtrlStreamBufferSizeInMB": 5
    },
    "ChangeProcessingDdlHandlingPolicy": {
      "HandleSourceTableDropped": true,
      "HandleSourceTableTruncated": true,
      "HandleSourceTableAltered": true
    },
    "ErrorBehavior": {
      "DataErrorPolicy": "LOG_ERROR",
      "DataTruncationErrorPolicy": "LOG_ERROR",
      "DataErrorEscalationPolicy": "SUSPEND_TABLE",
      "DataErrorEscalationCount": 0,
      "TableErrorPolicy": "SUSPEND_TABLE",
      "TableErrorEscalationPolicy": "STOP_TASK",
      "TableErrorEscalationCount": 0,
      "RecoverableErrorCount": -1,
      "RecoverableErrorInterval": 5,
      "RecoverableErrorThrottling": true,
      "RecoverableErrorThrottlingMax": 1800,
      "RecoverableErrorStopRetryAfterThrottlingMax": true,
      "ApplyErrorDeletePolicy": "IGNORE_RECORD",
      "ApplyErrorInsertPolicy": "LOG_ERROR",
      "ApplyErrorUpdatePolicy": "LOG_ERROR",
      "ApplyErrorEscalationPolicy": "LOG_ERROR",
      "ApplyErrorEscalationCount": 0,
      "ApplyErrorFailOnTruncationDdl": false,
      "FullLoadIgnoreConflicts": true,
      "FailOnTransactionConsistencyBreached": false,
      "FailOnNoTablesCaptured": true
    },
    "ChangeProcessingTuning": {
      "BatchApplyPreserveTransaction": true,
      "BatchApplyTimeoutMin": 1,
      "BatchApplyTimeoutMax": 30,
      "BatchApplyMemoryLimit": 500,
      "BatchSplitSize": 0,
      "MinTransactionSize": 1000,
      "CommitTimeout": 1,
      "MemoryLimitTotal": 1024,
      "MemoryKeepTime": 60,
      "StatementCacheSize": 50
    }
  }
}
```

## Part 2: S3 → PostgreSQL Batch Processing (Hourly)

### 2.1 Architecture for Batch Processing

```
S3 Bucket → AWS Glue Crawler → AWS Glue Job → PostgreSQL
    |              |                |              |
    └── Parquet ───┴── Catalog ─────┴── ETL ──────┘
```

### 2.2 AWS Glue Crawler Configuration

```json
{
  "Name": "orders-s3-crawler",
  "Role": "arn:aws:iam::123456789012:role/glue-crawler-role",
  "DatabaseName": "orders_database",
  "Description": "Crawler for orders CDC data in S3",
  "Targets": {
    "S3Targets": [
      {
        "Path": "s3://your-orders-bucket/orders-cdc/",
        "Exclusions": []
      }
    ]
  },
  "SchemaChangePolicy": {
    "UpdateBehavior": "UPDATE_IN_DATABASE",
    "DeleteBehavior": "DEPRECATE_IN_DATABASE"
  },
  "RecrawlPolicy": {
    "RecrawlBehavior": "CRAWL_EVERYTHING"
  },
  "LineageConfiguration": {
    "CrawlerLineageSettings": "ENABLE"
  },
  "Configuration": "{\"Version\":1.0,\"CrawlerOutput\":{\"Partitions\":{\"AddOrUpdateBehavior\":\"InheritFromTable\"},\"Tables\":{\"AddOrUpdateBehavior\":\"MergeNewColumns\"}}}"
}
```

### 2.3 AWS Glue ETL Job (PySpark)

```python
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql import functions as F
from pyspark.sql.types import *
import boto3

# Job parameters
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'TARGET_DB_HOST', 'TARGET_DB_NAME'])

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Read CDC data from S3
cdc_data = glueContext.create_dynamic_frame.from_catalog(
    database="orders_database",
    table_name="orders_cdc"
)

# Convert to DataFrame for processing
df = cdc_data.toDF()

# Filter for the last hour
current_hour = F.date_format(F.current_timestamp(), "yyyyMMddHH")
df_filtered = df.filter(
    F.date_format(F.col("dms_timestamp"), "yyyyMMddHH") == current_hour
)

# Process CDC operations
def process_cdc_operations(df):
    # Separate operations
    inserts = df.filter(F.col("Op") == "I").drop("Op")
    updates = df.filter(F.col("Op") == "U").drop("Op")
    deletes = df.filter(F.col("Op") == "D").select("order_id")
    
    return inserts, updates, deletes

inserts, updates, deletes = process_cdc_operations(df_filtered)

# Database connection properties
db_properties = {
    "user": "postgres_user",
    "password": "postgres_password",
    "driver": "org.postgresql.Driver"
}

jdbc_url = f"jdbc:postgresql://{args['TARGET_DB_HOST']}:5432/{args['TARGET_DB_NAME']}"

# Write operations to PostgreSQL
if inserts.count() > 0:
    inserts.write \
        .jdbc(url=jdbc_url, table="orders_staging_insert", mode="overwrite", properties=db_properties)

if updates.count() > 0:
    updates.write \
        .jdbc(url=jdbc_url, table="orders_staging_update", mode="overwrite", properties=db_properties)

if deletes.count() > 0:
    deletes.write \
        .jdbc(url=jdbc_url, table="orders_staging_delete", mode="overwrite", properties=db_properties)

job.commit()
```

### 2.4 PostgreSQL Merge Procedure

```sql
-- Create staging tables
CREATE TABLE IF NOT EXISTS orders_staging_insert (LIKE orders);
CREATE TABLE IF NOT EXISTS orders_staging_update (LIKE orders);
CREATE TABLE IF NOT EXISTS orders_staging_delete (order_id INTEGER);

-- Merge procedure
CREATE OR REPLACE FUNCTION merge_orders_from_s3()
RETURNS VOID AS $$
BEGIN
    -- Handle deletes first
    DELETE FROM orders 
    WHERE order_id IN (SELECT order_id FROM orders_staging_delete);
    
    -- Handle updates
    UPDATE orders SET
        customer_id = s.customer_id,
        order_date = s.order_date,
        total_amount = s.total_amount,
        status = s.status,
        updated_at = s.updated_at
    FROM orders_staging_update s
    WHERE orders.order_id = s.order_id;
    
    -- Handle inserts (avoid duplicates)
    INSERT INTO orders (order_id, customer_id, order_date, total_amount, status, created_at, updated_at)
    SELECT order_id, customer_id, order_date, total_amount, status, created_at, updated_at
    FROM orders_staging_insert s
    WHERE NOT EXISTS (SELECT 1 FROM orders WHERE order_id = s.order_id);
    
    -- Clear staging tables
    TRUNCATE orders_staging_insert, orders_staging_update, orders_staging_delete;
    
    -- Log the operation
    INSERT INTO replication_log (operation, timestamp, status)
    VALUES ('s3_to_postgres_merge', CURRENT_TIMESTAMP, 'completed');
    
END;
$$ LANGUAGE plpgsql;

-- Create log table
CREATE TABLE IF NOT EXISTS replication_log (
    id SERIAL PRIMARY KEY,
    operation VARCHAR(100),
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    status VARCHAR(50),
    records_processed INTEGER DEFAULT 0
);
```

### 2.5 EventBridge Rule for Hourly Execution

```json
{
  "Name": "orders-hourly-replication",
  "Description": "Trigger Glue job every hour for S3 to PostgreSQL replication",
  "ScheduleExpression": "rate(1 hour)",
  "State": "ENABLED",
  "Targets": [
    {
      "Id": "1",
      "Arn": "arn:aws:glue:region:account:job/orders-s3-to-postgres",
      "RoleArn": "arn:aws:iam::123456789012:role/eventbridge-glue-role",
      "GlueParameters": {
        "JobName": "orders-s3-to-postgres",
        "Arguments": {
          "--TARGET_DB_HOST": "your-postgres-host.amazonaws.com",
          "--TARGET_DB_NAME": "your_database"
        }
      }
    }
  ]
}
```

## Part 3: Complete AWS DMS JSON Configuration

### 3.1 Replication Instance

```json
{
  "ReplicationInstanceIdentifier": "orders-replication-instance",
  "ReplicationInstanceClass": "dms.t3.medium",
  "AllocatedStorage": 100,
  "VpcSecurityGroupIds": ["sg-12345678"],
  "ReplicationSubnetGroupIdentifier": "dms-subnet-group",
  "MultiAZ": false,
  "EngineVersion": "3.4.7",
  "AutoMinorVersionUpgrade": true,
  "PubliclyAccessible": false,
  "Tags": [
    {
      "Key": "Environment",
      "Value": "production"
    },
    {
      "Key": "Project",
      "Value": "orders-replication"
    }
  ]
}
```

### 3.2 IAM Role for DMS

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject",
        "s3:DeleteObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::your-orders-bucket",
        "arn:aws:s3:::your-orders-bucket/*"
      ]
    },
    {
      "Effect": "Allow",
      "Action": [
        "logs:CreateLogGroup",
        "logs:CreateLogStream",
        "logs:PutLogEvents",
        "logs:DescribeLogStreams"
      ],
      "Resource": "arn:aws:logs:*:*:*"
    }
  ]
}
```

## Part 4: Monitoring and Troubleshooting

### 4.1 CloudWatch Metrics

```json
{
  "MetricName": "ChangeEventCount",
  "Namespace": "AWS/DMS",
  "Dimensions": [
    {
      "Name": "ReplicationTaskArn",
      "Value": "arn:aws:dms:region:account:task:orders-cdc-stream"
    }
  ]
}
```

### 4.2 Error Handling

```sql
-- Create error log table
CREATE TABLE dms_error_log (
    id SERIAL PRIMARY KEY,
    table_name VARCHAR(100),
    operation VARCHAR(10),
    error_message TEXT,
    error_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    resolved BOOLEAN DEFAULT FALSE
);

-- Monitor CDC lag
SELECT 
    schemaname,
    tablename,
    n_tup_ins as inserts,
    n_tup_upd as updates,
    n_tup_del as deletes,
    last_vacuum,
    last_autovacuum
FROM pg_stat_user_tables 
WHERE tablename = 'orders';
```

## Conclusion

This setup provides:

1. **Real-time CDC streaming** from PostgreSQL to S3 using AWS DMS
2. **Hourly batch processing** from S3 back to PostgreSQL using AWS Glue
3. **Comprehensive monitoring** with CloudWatch and custom logging
4. **Error handling and recovery** mechanisms
5. **Scalable architecture** supporting high-volume data changes

The configuration ensures data consistency, handles schema changes, and provides audit trails for compliance requirements.